{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604fbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load libraries\n",
    "\n",
    "import torch as to\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f337c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Soft Indexing\n",
    "\n",
    "L = [[0.2, 0.5, 0.6],\n",
    "     [0.1, 0.0, 0.45],\n",
    "     [0.2, 0.7, 0.0]]\n",
    "\n",
    "M = np.array(L)\n",
    "SHAPE = (2,2)\n",
    "\n",
    "print (M)\n",
    "THRESHOLD_1 = 0.35\n",
    "args = np.argwhere(M>THRESHOLD_1)\n",
    "col_idx = np.unique(args[:,0])\n",
    "row_idx = np.unique(args[:,1])\n",
    "print (\"Undifferentiable spread :\", len(col_idx)+len(row_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278c88f-1b79-467f-a8d6-13f603cc4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARPNESS_1 = 50\n",
    "X = to.tensor(M, dtype=to.float32, requires_grad=True)\n",
    "mask = to.sigmoid((X - THRESHOLD_1) * SHARPNESS_1)\n",
    "print (mask)\n",
    "\n",
    "col_sum = mask.sum(dim=0)\n",
    "row_sum = mask.sum(dim=1)\n",
    "\n",
    "THRESHOLD_2 = 0.5\n",
    "SHARPNESS_2 = 5\n",
    "cols_idx = to.sigmoid((col_sum - THRESHOLD_2) * SHARPNESS_2)\n",
    "n_cols = cols_idx.sum()\n",
    "rows_idx = to.sigmoid((row_sum - THRESHOLD_2) * SHARPNESS_2)\n",
    "n_rows = rows_idx.sum()\n",
    "spread = n_cols + n_rows\n",
    "loss = (spread - SHAPE[0] - SHAPE[1])\n",
    "print (f\"Differentiable spread :\", spread)\n",
    "print (f\"Differentiable loss   :\", loss)\n",
    "\n",
    "print (col_sum) # approximation of the amount of items per column / row\n",
    "print (to.sigmoid((col_sum - THRESHOLD_2) * SHARPNESS_2))\n",
    "first_col_idx = getFirstIdx(cols_idx)\n",
    "print (\"First col index\", first_col_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70273de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compactness_loss(X, SHAPE, T1, S1, T2, S2): # X 2D\n",
    "    mask = to.sigmoid((X - T1) * S1)\n",
    "    col_sum = mask.sum(dim=0)\n",
    "    row_sum = mask.sum(dim=1)\n",
    "    n_cols = to.sigmoid((col_sum - T2) * S2).sum()\n",
    "    n_rows = to.sigmoid((row_sum - T2) * S2).sum()\n",
    "    spread = n_cols + n_rows\n",
    "    loss = (spread - SHAPE[0] - SHAPE[1])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a52bc-41bb-4977-85a8-19c07770b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, _ = to.topk(X.view(-1), 4)\n",
    "to.min(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc735279-4962-4a7c-b4a3-0dcc6361a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMin(X, K): # X 2D\n",
    "    v, _ = to.topk(X.view(-1), K)\n",
    "    return v.min()\n",
    "\n",
    "print (getMin(X, 4))\n",
    "\n",
    "def getMin2(X, K):\n",
    "    sorted_X, _ = torch.sort(X, descending=True)\n",
    "    return sorted_X  # Differentiable alternative\n",
    "\n",
    "print (getMin2(X, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bfef8-d63b-4a2f-a87b-330ba17ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor\n",
    "X= to.tensor([0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], requires_grad=True)\n",
    "\n",
    "def getFirstIdx(X):\n",
    "    excess = to.relu(X - 0.9)  # Values below the threshold will be 0\n",
    "    indices = to.arange(len(X), dtype=to.float32)\n",
    "    decay = 1 / (indices + 1)  # Decay factor: higher for earlier indices\n",
    "    weighted_excess = excess * decay  # Combine excess with decay\n",
    "    weights = to.softmax(weighted_excess * 1000, dim=0)  # Scale by a factor (10) to sharpen the selection\n",
    "    soft_index = to.sum(indices * weights)\n",
    "    return soft_index\n",
    "\n",
    "def getLastIdx(X):\n",
    "    excess = to.relu(X - 0.9)  # Values below the threshold will be 0\n",
    "    indices = to.arange(len(X), dtype=to.float32)\n",
    "    reverse_decay = indices + 1  # Decay factor: higher for earlier indices\n",
    "    weighted_excess = excess * reverse_decay  # Combine excess with decay\n",
    "    weights = to.softmax(weighted_excess * 1000, dim=0)  # Scale by a factor (10) to sharpen the selection\n",
    "    print (weights)\n",
    "    soft_index = to.sum(indices * weights)\n",
    "    return soft_index\n",
    "\n",
    "def getSpan(X):\n",
    "    first = getFirstIdx(X)\n",
    "    last = getLastIdx(X)\n",
    "    return last-first\n",
    "\n",
    "first_index = getFirstIdx(X)\n",
    "last_index = getLastIdx(X)\n",
    "\n",
    "print(f\"First index: {first_index.item():.4f}\")\n",
    "print(f\"Last index: {last_index.item():.4f}\")\n",
    "print(f\"Span :\", getSpan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "928e0942-0181-4aea-8b38-90a94719116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([25, 10, 15, 20, 30])\n",
      "Epoch      1/  5000 | Loss:    19.2516, T°  : 100.0\n",
      "Epoch    501/  5000 | Loss:     6.8261, T°  : 4.468\n",
      "Epoch   1001/  5000 | Loss:     6.8258, T°  : 3.161\n",
      "Epoch   1501/  5000 | Loss:     6.8783, T°  : 2.581\n",
      "Epoch   2001/  5000 | Loss:     6.8783, T°  : 2.236\n",
      "Epoch   2501/  5000 | Loss:     7.9845, T°  : 2.0\n",
      "Epoch   3001/  5000 | Loss:     7.8201, T°  : 1.825\n",
      "Epoch   3501/  5000 | Loss:     7.7458, T°  : 1.69\n",
      "Epoch   4001/  5000 | Loss:     7.6827, T°  : 1.581\n",
      "Epoch   4501/  5000 | Loss:     7.6801, T°  : 1.491\n",
      "tensor([1.6724e-01, 1.4164e-01, 1.0000e+00, 9.9999e-01, 4.5049e-06, 1.0000e+00,\n",
      "        1.0377e-01, 1.0000e+00, 2.0000e-01, 8.7901e-02],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "epoch 4999, shape (5, 5)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([1.6724e-01, 1.4164e-01, 1.0000e+00, 9.9999e-01, 4.5049e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Row loss       0.4936439096927643 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([1.0000e-06, 1.0000e-06, 1.0000e+00, 1.0000e+00, 1.0000e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             CS        tensor([1.0000e-05, 1.0000e-05, 2.0000e+00, 3.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             RS        tensor([4.0000e+00, 1.0000e-05, 2.0000e+00, 1.0000e-05],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "      col_idx 1        tensor([0.0759, 0.0759, 0.9994, 1.0000, 0.0759, 0.9994, 0.0759, 0.9994, 0.0759,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      col_idx 2        tensor([0.0759, 0.0759, 0.9994, 1.0000, 0.0759, 0.9994, 0.0759, 0.9994, 0.0759,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      span col         3.5397708415985107\n",
      "      span row         3.7729690074920654\n",
      "      span loss        0.7312740087509155\n",
      "      spread           7.983983993530273\n",
      "      spread loss      0.7983983755111694\n",
      "      row loss         1.529672384262085\n",
      "\n",
      "epoch 4999, shape (5, 2)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([2.2475e-02, 4.4550e-01, 6.7059e-08, 7.0717e-07, 3.5768e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Row loss       0.6161945462226868 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             CS        tensor([1.0000e-05, 2.0000e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             RS        tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "      col_idx 1        tensor([0.0759, 0.9994, 0.0759, 0.0759, 0.9241, 0.0759, 0.0759, 0.0759, 0.0759,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      col_idx 2        tensor([0.0759, 0.9994, 0.0759, 0.0759, 0.9241, 0.0759, 0.0759, 0.0759, 0.0759,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      span col         2.014131546020508\n",
      "      span row         -0.3348062038421631\n",
      "      span loss        0.23990361392498016\n",
      "      spread           5.060971260070801\n",
      "      spread loss      0.7229958772659302\n",
      "      row loss         0.9628995060920715\n",
      "\n",
      "epoch 4999, shape (3, 5)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([2.2512e-02, 1.3473e-01, 6.5159e-08, 4.7482e-06, 9.9998e-01],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Row loss       0.7818853855133057 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             CS        tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             RS        tensor([1.0000e+00, 2.0000e+00, 1.0000e-05, 3.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "      col_idx 1        tensor([0.0759, 0.0759, 0.0759, 0.9241, 1.0000, 0.0759, 0.0759, 0.9241, 0.0759,\n",
      "        0.9994], grad_fn=<SigmoidBackward0>)\n",
      "      col_idx 2        tensor([0.0759, 0.0759, 0.0759, 0.9241, 1.0000, 0.0759, 0.0759, 0.9241, 0.0759,\n",
      "        0.9994], grad_fn=<SigmoidBackward0>)\n",
      "      span col         4.2485880851745605\n",
      "      span row         1.774239420890808\n",
      "      span loss        0.7528534531593323\n",
      "      spread           8.605806350708008\n",
      "      spread loss      1.075725793838501\n",
      "      row loss         1.8285791873931885\n",
      "\n",
      "epoch 4999, shape (2, 10)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([7.6504e-01, 1.4191e-01, 2.5769e-06, 1.5658e-06, 4.0372e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Row loss       0.8729608654975891 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([2.0564e-05, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             CS        tensor([2.0922e+00, 1.0000e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             RS        tensor([3.8177e-05, 1.0000e-05, 3.8161e-05, 1.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "      col_idx 1        tensor([0.9997, 0.9241, 0.0759, 0.0759, 0.0759, 0.0759, 0.9994, 0.0759, 1.0000,\n",
      "        0.9242], grad_fn=<SigmoidBackward0>)\n",
      "      col_idx 2        tensor([0.9997, 0.9241, 0.0759, 0.0759, 0.0759, 0.0759, 0.9994, 0.0759, 1.0000,\n",
      "        0.9242], grad_fn=<SigmoidBackward0>)\n",
      "      span col         7.7616095542907715\n",
      "      span row         3.159024238586426\n",
      "      span loss        0.9100527763366699\n",
      "      spread           9.556839942932129\n",
      "      spread loss      0.7964033484458923\n",
      "      row loss         1.706456184387207\n",
      "\n",
      "epoch 4999, shape (10, 3)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([2.2732e-02, 1.3622e-01, 6.4914e-08, 8.1921e-07, 3.9144e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Row loss       0.8434552550315857 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             CS        tensor([1.0000e-05, 1.0000e-05, 5.0000e+00, 2.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "             RS        tensor([1.0000e-05, 1.0000e-05, 5.0000e+00, 1.0000e-05],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "      col_idx 1        tensor([0.0759, 0.0759, 1.0000, 0.9994, 1.0000, 1.0000, 0.9994, 0.9241, 1.0000,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      col_idx 2        tensor([0.0759, 0.0759, 1.0000, 0.9994, 1.0000, 1.0000, 0.9994, 0.9241, 1.0000,\n",
      "        0.0759], grad_fn=<SigmoidBackward0>)\n",
      "      span col         4.185733795166016\n",
      "      span row         3.9182546138763428\n",
      "      span loss        0.6233837604522705\n",
      "      spread           13.378204345703125\n",
      "      spread loss      1.0290926694869995\n",
      "      row loss         1.65247642993927\n",
      "tensor([[3, 1, 0, 0, 2, 0, 2, 0, 3, 3],\n",
      "        [4, 2, 1, 2, 2, 4, 4, 4, 4, 2],\n",
      "        [3, 4, 4, 0, 4, 4, 4, 0, 4, 3],\n",
      "        [1, 1, 2, 2, 1, 3, 1, 2, 3, 2],\n",
      "        [3, 3, 0, 0, 2, 0, 3, 2, 3, 3],\n",
      "        [4, 4, 4, 2, 4, 4, 4, 2, 4, 3],\n",
      "        [1, 1, 4, 2, 4, 4, 2, 1, 2, 0],\n",
      "        [1, 2, 4, 4, 0, 4, 0, 1, 4, 3],\n",
      "        [3, 3, 2, 4, 3, 3, 3, 4, 4, 1],\n",
      "        [3, 1, 4, 1, 2, 3, 4, 4, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "BOARD_SIZE = (10,10)\n",
    "board = np.zeros(BOARD_SIZE).astype(int)\n",
    "INDEXES = list(zip(*np.where(board == 0)))\n",
    "\n",
    "SHAPES = [(5,5), (5,2), (3,5), (2,10), (10,3)]\n",
    "print (sum(SHAPES[0]))\n",
    "shapes = [np.ones(shape).astype(int) for shape in SHAPES]\n",
    "\n",
    "NCELLS = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
    "NSHAPES = len(shapes)\n",
    "BOARD_TOTAL = sum([(i+1)*np.sum(shape) for i, shape in enumerate(shapes)])\n",
    "K = torch.tensor([np.sum(s) for s in shapes])\n",
    "print (K)\n",
    "\n",
    "class FacilityLayoutNet(nn.Module):\n",
    "    def __init__(self, NSHAPES, NCELLS, initial_temperature, PARAM_INDEXES):\n",
    "        super(FacilityLayoutNet, self).__init__()\n",
    "        self.NSHAPES = NSHAPES\n",
    "        self.NCELLS = NCELLS\n",
    "        self.PARAM_INDEXES = PARAM_INDEXES\n",
    "        \n",
    "        #initials = torch.randn((NSHAPES, NCELLS), dtype=torch.float32) * 1.\n",
    "        initials = torch.empty((NSHAPES, NCELLS)).uniform_(-0.1, 0.1)\n",
    "        #initials = self.boltzmann_probability(initials, initial_temperature)\n",
    "        self.logits = nn.Parameter(initials)\n",
    "        \n",
    "    def boltzmann_probability(self, E, T):\n",
    "        exp_neg_E = torch.exp(-E / T - torch.max(-E / T))  # Stabilized\n",
    "        partition_function = torch.sum(exp_neg_E)\n",
    "        return exp_neg_E / partition_function\n",
    "\n",
    "    def forward(self, temperature=1.0):\n",
    "        temperature = min(1., temperature)\n",
    "        probabilities = torch.softmax(self.logits, dim=0) # / temperature\n",
    "        return probabilities\n",
    "\"\"\"   \n",
    "def getMin(X, K): # X 1D\n",
    "    v, _ = torch.topk(X, K)\n",
    "    return v.min()\n",
    "\"\"\"\n",
    "def getMin(X, K):\n",
    "    sorted_X, _ = torch.sort(X, descending=True)\n",
    "    return sorted_X[:K].mean()  # Differentiable alternative\n",
    " \n",
    "def getMinima(X, K): # X 2D, K list\n",
    "    minima = []\n",
    "    for i in range(X.size(0)):\n",
    "        minima.append(getMin(X[i,:], K[i])) # X 1D\n",
    "    return minima\n",
    "\n",
    "def row_loss(X, SHAPE, T1, S1, T2, S2, verbose=False):  # X 2D, row reshaped\n",
    "    # Apply sigmoid with clamping to avoid saturation\n",
    "    mask = torch.sigmoid((X - T1) * S1)\n",
    "    mask = torch.clamp(mask, min=1e-6, max=1 - 1e-6)\n",
    "    if verbose: print (f\"             M1        {mask[0, :5]}\")\n",
    "\n",
    "    # Compute column and row sums\n",
    "    col_sum = mask.sum(dim=0)\n",
    "    row_sum = mask.sum(dim=1)\n",
    "    if verbose: print (f\"             CS        {col_sum[:4]}\")\n",
    "    if verbose: print (f\"             RS        {row_sum[:4]}\")\n",
    "\n",
    "    # Apply clamping to sigmoid output for col and row indices\n",
    "    col_idx = torch.sigmoid((col_sum - T2) * S2)\n",
    "    if verbose: print (f\"      col_idx 1        {col_idx}\")\n",
    "\n",
    "    row_idx = torch.sigmoid((row_sum - T2) * S2)\n",
    "    #col_idx = torch.clamp(col_idx, min=1e-6, max=1 - 1e-6)\n",
    "    #row_idx = torch.clamp(row_idx, min=1e-6, max=1 - 1e-6)\n",
    "    if verbose: print (f\"      col_idx 2        {col_idx}\")\n",
    "\n",
    "    # Calculate span and spread\n",
    "    col_span = getSpan(col_idx)\n",
    "    if verbose: print (f\"      span col         {col_span}\")\n",
    "    row_span = getSpan(row_idx)\n",
    "    if verbose: print (f\"      span row         {row_span}\")\n",
    "    span = col_span + row_span\n",
    "    max_expected = sum(SHAPE)\n",
    "    span_loss = span / max_expected\n",
    "    if verbose: print (f\"      span loss        {span_loss}\")\n",
    "\n",
    "    # Spread is the sum of the number of columns and rows involved\n",
    "    n_cols = col_idx.sum()\n",
    "    n_rows = row_idx.sum()\n",
    "    spread = n_cols + n_rows\n",
    "    spread_loss = spread / max_expected\n",
    "    if verbose: print (f\"      spread           {spread}\")\n",
    "    if verbose: print (f\"      spread loss      {spread_loss}\")\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = span_loss + spread_loss\n",
    "    if verbose: print (f\"      row loss         {loss}\")\n",
    "    return loss\n",
    "\n",
    "def get_amount_loss(X, K, T1s, S1, verbose=False):\n",
    "    total_loss = 0.\n",
    "    for i in range(len(K)):\n",
    "        row_X = X[i,:]\n",
    "        target = K[i]\n",
    "        T1 = T1s[i]\n",
    "        mask = torch.sigmoid((X - T1) * S1)\n",
    "        loss = (mask.sum() - K[i])**2\n",
    "        total_loss += loss.abs() / K[i]\n",
    "    return total_loss\n",
    "\n",
    "def entropy_loss(probabilities):\n",
    "    # Entropy: H(p) = -sum(p * log(p))\n",
    "    eps = 1e-6  # Avoid log(0)\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities + eps), dim=0)\n",
    "    return entropy.mean()\n",
    "    \n",
    "def compactness_loss(X, BOARD_SIZE, SHAPES, T1s, S1, T2, S2, e, verbose=False): # X as main probaility matrix\n",
    "    total_loss = 0.\n",
    "    for i in range(len(SHAPES)):\n",
    "        if verbose: print (f\"\\nepoch {e}, shape {SHAPES[i]}  ====================================\")\n",
    "        shape_X = X[i,:].view(BOARD_SIZE)\n",
    "        if verbose: print (f\"shape_X        {shape_X.shape}\")\n",
    "        if verbose: print (f\"shape_X[0,:5]  {shape_X[0,:5]}\")\n",
    "        shape = SHAPES[i]\n",
    "        T1 = T1s[i]\n",
    "        if verbose: print (f\"Row loss       {T1} - {S1} - {T2} - {S2}\")\n",
    "        loss = row_loss(shape_X, shape, T1, S1, T2, S2, verbose)\n",
    "        \n",
    "        total_loss += loss.abs()\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def getFirstIdx(X):\n",
    "    excess = torch.relu(X - 0.9)  # Values below the threshold will be 0\n",
    "    indices = torch.arange(len(X), dtype=torch.float32)\n",
    "    decay = 1 / (indices + 1)  # Decay factor: higher for earlier indices\n",
    "    weighted_excess = excess * decay  # Combine excess with decay\n",
    "    weights = torch.softmax(weighted_excess * 100, dim=0)  # Scale by a factor (10) to sharpen the selection\n",
    "    soft_index = torch.sum(indices * weights)\n",
    "    return soft_index\n",
    "\n",
    "def getLastIdx(X):\n",
    "    excess = torch.relu(X - 0.9)  # Values below the threshold will be 0\n",
    "    indices = torch.arange(len(X), dtype=torch.float32)\n",
    "    reverse_decay = (indices + 1) / len(X)  # Decay factor: higher for earlier indices\n",
    "    weighted_excess = excess * reverse_decay  # Combine excess with decay\n",
    "    weights = torch.softmax(weighted_excess * 100, dim=0)  # Scale by a factor (10) to sharpen the selection\n",
    "    soft_index = torch.sum(indices * weights)\n",
    "    return soft_index\n",
    "\n",
    "def getSpan(X):\n",
    "    first = getFirstIdx(X)\n",
    "    last = getLastIdx(X)\n",
    "    return last-first\n",
    "\n",
    "# Training loop\n",
    "PRINT_MOD = 50\n",
    "INITIAL_TEMPERATURE = 100.\n",
    "FINAL_TEMPERATURE = 1.\n",
    "PARAM_INDEXES = np.where(np.zeros((NSHAPES, NCELLS))== 0)\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "# Initialize the model\n",
    "model = FacilityLayoutNet(NSHAPES, NCELLS, INITIAL_TEMPERATURE, PARAM_INDEXES)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, momentum=0.9)\n",
    "PARAM_INDEXES = np.where(np.zeros((NSHAPES, NCELLS))== 0)\n",
    "\n",
    "all_params = list(model.parameters())\n",
    "num_params = len(all_params)\n",
    "losses = []\n",
    "\n",
    "def train(num_epochs=100, IT=100., PRINT_MOD=PRINT_MOD):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"(1) {name} grad mean: {param.grad.abs().mean()}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        temperature = num_epochs / ((epoch+1) ** 0.5)\n",
    "        temperature = (temperature / num_epochs) * IT \n",
    "        probabilities = model(temperature)\n",
    "        if epoch == num_epochs-1:\n",
    "            print (probabilities[0,:10])\n",
    "        \n",
    "        # T1s = getMinima(probabilities, K) # list of row-wise minima with regard to K[i]\n",
    "        T1s = torch.stack([getMin(probabilities[i, :], K[i]) for i in range(len(K))])\n",
    "        T2 = 0.5\n",
    "        S1 = 100.\n",
    "        S2 = 5.\n",
    "        if epoch == num_epochs-1:\n",
    "            comp_loss = compactness_loss(probabilities, BOARD_SIZE, SHAPES, T1s, S1, T2, S2, epoch, verbose=True)\n",
    "        else:\n",
    "            comp_loss = compactness_loss(probabilities, BOARD_SIZE, SHAPES, T1s, S1, T2, S2, epoch, verbose=False)\n",
    "        amount_loss = get_amount_loss(probabilities, K, T1s, S1)\n",
    "        entropy = entropy_loss(probabilities)\n",
    "        total_loss = (\n",
    "            comp_loss\n",
    "            - (1e-05 * entropy)\n",
    "        )\n",
    "    \n",
    "        losses.append(total_loss.item())\n",
    "        total_loss.backward()\n",
    "    \n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"(2) {name} grad mean: {param.grad.abs().mean()}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch) % PRINT_MOD == 0:\n",
    "            s1 = f\"Epoch {epoch + 1 : 6d}/{num_epochs : 6d}\"\n",
    "            s2 = f\"Loss: {total_loss.item() : 10.4f}\"\n",
    "            s3 = f\"T°  : {round(temperature,3)}\"\n",
    "            print(f\"{s1} | {s2}, {s3}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, temperature, probabilities.detach()\n",
    "\n",
    "model, temperature, probabilities = train(num_epochs=5000, IT=100, PRINT_MOD=500)\n",
    "\n",
    "assign0 = torch.argmax(probabilities, dim=0)\n",
    "\n",
    "print (assign0.reshape(BOARD_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "9cb4b19c-0c70-496c-9a2e-4f0f5af6eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "d3151724-a16b-4289-8d26-64ea45d91b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 0 0 2 0 2 0 3 3]\n",
      " [4 2 1 2 2 4 4 4 4 2]\n",
      " [3 4 4 0 4 4 4 0 4 3]\n",
      " [1 1 2 2 1 3 1 2 3 2]\n",
      " [3 3 0 0 2 0 3 2 3 3]\n",
      " [4 4 4 2 4 4 4 2 4 3]\n",
      " [1 1 4 2 4 4 2 1 2 0]\n",
      " [1 2 4 4 0 4 0 1 4 3]\n",
      " [3 3 2 4 3 3 3 4 4 1]\n",
      " [3 1 4 1 2 3 4 4 3 3]]\n",
      "(5, 5) 25 tensor(25) 25\n",
      "[[' ' ' ' 'X' 'X' ' ' 'X' ' ' 'X' ' ' ' ']\n",
      " [' ' 'X' 'X' 'X' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'X' ' ' ' ' ' ' 'X' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'X' 'X' 'X' ' ' 'X' ' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' 'X' ' ' ' ' ' ' ' ' ' ' 'X']\n",
      " ['X' ' ' ' ' ' ' ' ' ' ' 'X' ' ' 'X' 'X']\n",
      " [' ' ' ' ' ' ' ' 'X' ' ' 'X' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'X' ' ' ' ' 'X' 'X' ' ' ' ']]\n",
      "tensor([0.4936])\n",
      "[[' ' ' ' 'X' 'X' ' ' 'X' ' ' 'X' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'X' ' ' ' ' ' ' 'X' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' 'X' ' ' 'X' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n",
      "\n",
      "epoch 0, shape (5, 5)  ====================================\n",
      "shape_X        torch.Size([10, 10])\n",
      "shape_X[0,:5]  tensor([1.6724e-01, 1.4164e-01, 1.0000e+00, 9.9999e-01, 4.5049e-06])\n",
      "Row loss       0.4936439096927643 - 100.0 - 0.5 - 5.0\n",
      "             M1        tensor([1.0000e-06, 1.0000e-06, 1.0000e+00, 1.0000e+00, 1.0000e-06])\n",
      "             CS        tensor([1.0000e-05, 1.0000e-05, 2.0000e+00, 3.0000e+00])\n",
      "             RS        tensor([4.0000e+00, 1.0000e-05, 2.0000e+00, 1.0000e-05])\n",
      "      col_idx 1        tensor([0.0759, 0.0759, 0.9994, 1.0000, 0.0759, 0.9994, 0.0759, 0.9994, 0.0759,\n",
      "        0.0759])\n",
      "      col_idx 2        tensor([0.0759, 0.0759, 0.9994, 1.0000, 0.0759, 0.9994, 0.0759, 0.9994, 0.0759,\n",
      "        0.0759])\n",
      "      span col         3.5397708415985107\n",
      "      span row         3.7729690074920654\n",
      "      span loss        0.7312740087509155\n",
      "      spread           7.983983993530273\n",
      "      spread loss      0.7983983755111694\n",
      "      row loss         1.529672384262085\n",
      "tensor(1.5297) tensor(10.2400)\n"
     ]
    }
   ],
   "source": [
    "result = probabilities.numpy()\n",
    "arr0 = np.zeros(NCELLS).astype(object)\n",
    "\n",
    "n = 0\n",
    "s1 = np.argsort(result[n])[-K[n]:]\n",
    "s2 = np.argsort(result[n])[:-K[n]]\n",
    "\n",
    "arr0[s1] = \"X\"\n",
    "arr0[s2] = \" \"\n",
    "arr0 = arr0.reshape(BOARD_SIZE)\n",
    "\n",
    "assign0 = torch.argmax(probabilities, dim=0).detach().numpy()\n",
    "print (assign0.reshape(BOARD_SIZE))\n",
    "\n",
    "print (SHAPES[n], SHAPES[n][0]*SHAPES[n][1], K[n], len(s1))\n",
    "print (arr0)\n",
    "\n",
    "T1s = torch.stack([getMin(probabilities[i, :], K[i]) for i in [n]])\n",
    "print (T1s)\n",
    "mask = (probabilities[n, :].numpy()>T1s.item()).reshape(BOARD_SIZE)\n",
    "arr0[mask] = 'X'\n",
    "arr0[~mask] = ' '\n",
    "print (arr0)\n",
    "T2 = 0.5\n",
    "S1 = 100.\n",
    "S2 = 5.\n",
    "comp_loss = compactness_loss(probabilities[n, :].unsqueeze(0), BOARD_SIZE, [SHAPES[n]], T1s, S1, T2, S2, 0, verbose=True)\n",
    "amount_loss = get_amount_loss(probabilities[n, :].unsqueeze(0), [K[n]], T1s, S1)\n",
    "print (comp_loss, amount_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "2d8bacf7-d0f6-4058-a31c-01bdb33c3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "id": "391dcca1-5e23-43fd-b3b4-ef757d1ac9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sum -0.9689087867736816\n",
      "Data avg -0.009689087979495525\n",
      "Data span 4.910699844360352\n",
      "e  2.71828\n",
      "tensor([ 0.2940,  5.0090,  0.8850,  2.4367,  0.3734,  4.0135, 11.4558,  1.0716,\n",
      "         0.9487,  0.4891,  0.3170,  1.0213,  0.6511,  0.5852,  0.6434,  4.6950,\n",
      "         1.9216,  1.4166,  2.4909,  0.4250,  2.4028,  0.5811,  0.1690,  0.3956,\n",
      "         1.5333,  1.2003,  0.2609,  1.5271,  2.4095,  0.3918,  1.6051,  3.1364,\n",
      "         6.1607,  0.3685,  0.7392,  1.6748,  1.2543,  0.5398,  2.4191,  0.2435,\n",
      "         0.1488,  0.7794,  0.5792,  1.1166,  1.8965,  1.1452,  0.4452,  0.3017,\n",
      "         0.7410,  1.8880,  1.2718,  0.8680,  0.2929,  0.2812,  0.4603,  0.2112,\n",
      "         0.2864,  4.4266,  0.4257,  0.1543,  1.2669,  0.8052, 11.2810,  0.4054,\n",
      "         1.5832,  1.3719,  1.6186,  1.6603,  0.1750,  0.4694,  0.1759,  2.6190,\n",
      "         1.2233, 20.1932,  0.4097,  1.9104,  2.6381,  0.2505,  0.5096,  0.5843,\n",
      "         4.3491,  1.3524,  0.6942,  6.1504,  0.4512,  2.3681,  0.3538,  1.6643,\n",
      "         1.6254,  0.7053,  0.3346,  1.5009,  2.9984,  2.3759,  0.9126,  1.2859,\n",
      "         3.0541,  0.7222,  1.0477,  2.3019])\n",
      "tensor([0.0017, 0.0286, 0.0050, 0.0139, 0.0021, 0.0229, 0.0654, 0.0061, 0.0054,\n",
      "        0.0028, 0.0018, 0.0058, 0.0037, 0.0033, 0.0037, 0.0268, 0.0110, 0.0081,\n",
      "        0.0142, 0.0024, 0.0137, 0.0033, 0.0010, 0.0023, 0.0087, 0.0068, 0.0015,\n",
      "        0.0087, 0.0137, 0.0022, 0.0092, 0.0179, 0.0351, 0.0021, 0.0042, 0.0096,\n",
      "        0.0072, 0.0031, 0.0138, 0.0014, 0.0008, 0.0044, 0.0033, 0.0064, 0.0108,\n",
      "        0.0065, 0.0025, 0.0017, 0.0042, 0.0108, 0.0073, 0.0050, 0.0017, 0.0016,\n",
      "        0.0026, 0.0012, 0.0016, 0.0253, 0.0024, 0.0009, 0.0072, 0.0046, 0.0644,\n",
      "        0.0023, 0.0090, 0.0078, 0.0092, 0.0095, 0.0010, 0.0027, 0.0010, 0.0149,\n",
      "        0.0070, 0.1152, 0.0023, 0.0109, 0.0151, 0.0014, 0.0029, 0.0033, 0.0248,\n",
      "        0.0077, 0.0040, 0.0351, 0.0026, 0.0135, 0.0020, 0.0095, 0.0093, 0.0040,\n",
      "        0.0019, 0.0086, 0.0171, 0.0136, 0.0052, 0.0073, 0.0174, 0.0041, 0.0060,\n",
      "        0.0131])\n",
      "tensor([0.0017, 0.0286, 0.0050, 0.0139, 0.0021, 0.0229, 0.0654, 0.0061, 0.0054,\n",
      "        0.0028, 0.0018, 0.0058, 0.0037, 0.0033, 0.0037, 0.0268, 0.0110, 0.0081,\n",
      "        0.0142, 0.0024, 0.0137, 0.0033, 0.0010, 0.0023, 0.0087, 0.0068, 0.0015,\n",
      "        0.0087, 0.0137, 0.0022, 0.0092, 0.0179, 0.0351, 0.0021, 0.0042, 0.0096,\n",
      "        0.0072, 0.0031, 0.0138, 0.0014, 0.0008, 0.0044, 0.0033, 0.0064, 0.0108,\n",
      "        0.0065, 0.0025, 0.0017, 0.0042, 0.0108, 0.0073, 0.0050, 0.0017, 0.0016,\n",
      "        0.0026, 0.0012, 0.0016, 0.0253, 0.0024, 0.0009, 0.0072, 0.0046, 0.0644,\n",
      "        0.0023, 0.0090, 0.0078, 0.0092, 0.0095, 0.0010, 0.0027, 0.0010, 0.0149,\n",
      "        0.0070, 0.1152, 0.0023, 0.0109, 0.0151, 0.0014, 0.0029, 0.0033, 0.0248,\n",
      "        0.0077, 0.0040, 0.0351, 0.0026, 0.0135, 0.0020, 0.0095, 0.0093, 0.0040,\n",
      "        0.0019, 0.0086, 0.0171, 0.0136, 0.0052, 0.0073, 0.0174, 0.0041, 0.0060,\n",
      "        0.0131])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print (data)\n",
    "print (f\"Data sum {data.sum().item()}\")\n",
    "print (f\"Data avg {data.mean().item()}\")\n",
    "print (f\"Data span {data.max()-data.min()}\")\n",
    "print (f\"e {np.e: 2.5f}\")\n",
    "\n",
    "data_e = torch.e ** data\n",
    "print (data_e)\n",
    "print (data_e / data_e.sum())\n",
    "print (torch.softmax(data, dim=0))\n",
    "\n",
    "_ = plt.hist(data, bins=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "id": "cbc95790-3ef6-4aef-9020-bd965e19cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9997.1475)\n",
      "tensor(0.)\n",
      "topKMask\n",
      "100 10000 -51 tensor(53.2705)\n",
      "100 10000 -50 tensor(53.8966)\n",
      "100 10000 -49 tensor(54.4518)\n",
      "100 10000 -48 tensor(54.8748)\n",
      "100 10000 -47 tensor(55.2418)\n",
      "100 10000 -46 tensor(55.6787)\n",
      "100 10000 -45 tensor(56.2268)\n",
      "100 10000 -44 tensor(56.8680)\n",
      "100 10000 -43 tensor(57.6307)\n",
      "100 10000 -42 tensor(58.5683)\n",
      "100 10000 -41 tensor(59.6116)\n",
      "100 10000 -40 tensor(60.6167)\n",
      "100 10000 -39 tensor(61.4811)\n",
      "100 10000 -38 tensor(62.2159)\n",
      "100 10000 -37 tensor(62.9252)\n",
      "100 10000 -36 tensor(63.6233)\n",
      "100 10000 -35 tensor(64.3274)\n",
      "100 10000 -34 tensor(65.2081)\n",
      "100 10000 -33 tensor(66.3153)\n",
      "100 10000 -32 tensor(67.3875)\n",
      "100 10000 -31 tensor(68.2594)\n",
      "100 10000 -30 tensor(69.0234)\n",
      "100 10000 -29 tensor(69.8189)\n",
      "100 10000 -28 tensor(70.7588)\n",
      "100 10000 -27 tensor(71.9261)\n",
      "100 10000 -26 tensor(73.3556)\n",
      "100 10000 -25 tensor(74.9885)\n",
      "100 10000 -24 tensor(76.7507)\n",
      "100 10000 -23 tensor(78.5796)\n",
      "100 10000 -22 tensor(80.3544)\n",
      "100 10000 -21 tensor(81.9666)\n",
      "100 10000 -20 tensor(83.4002)\n",
      "100 10000 -19 tensor(84.7433)\n",
      "100 10000 -18 tensor(86.1740)\n",
      "100 10000 -17 tensor(87.8194)\n",
      "100 10000 -16 tensor(89.5715)\n",
      "100 10000 -15 tensor(91.1854)\n",
      "100 10000 -14 tensor(92.5479)\n",
      "100 10000 -13 tensor(93.6460)\n",
      "100 10000 -12 tensor(94.5762)\n",
      "100 10000 -11 tensor(95.5333)\n",
      "100 10000 -10 tensor(96.6665)\n",
      "100 10000 -9 tensor(97.8835)\n",
      "100 10000 -8 tensor(98.8878)\n",
      "100 10000 -7 tensor(99.5049)\n",
      "100 10000 -6 tensor(99.8017)\n",
      "100 10000 -5 tensor(99.9245)\n",
      "100 10000 -4 tensor(99.9719)\n",
      "100 10000 -3 tensor(99.9896)\n",
      "100 10000 -2 tensor(99.9962)\n",
      "100 10000 -1 tensor(99.9986)\n",
      "100 10000 0 tensor(99.9995)\n",
      "100 10000 1 tensor(99.9998)\n",
      "100 10000 2 tensor(99.9999)\n",
      "100 10000 3 tensor(100.0000)\n",
      "100 10000 4 tensor(100.0000)\n",
      "100 10000 5 tensor(100.)\n",
      "100 10000 6 tensor(100.)\n",
      "100 10000 7 tensor(100.)\n",
      "100 10000 8 tensor(100.)\n",
      "100 10000 9 tensor(100.)\n",
      "100 10000 10 tensor(100.)\n",
      "100 10000 11 tensor(100.)\n",
      "100 10000 12 tensor(100.)\n",
      "100 10000 13 tensor(100.)\n",
      "100 10000 14 tensor(100.)\n",
      "100 10000 15 tensor(100.)\n",
      "100 10000 16 tensor(100.)\n",
      "100 10000 17 tensor(100.)\n",
      "100 10000 18 tensor(100.)\n",
      "100 10000 19 tensor(100.)\n",
      "100 10000 20 tensor(100.)\n",
      "100 10000 21 tensor(100.)\n",
      "100 10000 22 tensor(100.)\n",
      "100 10000 23 tensor(100.)\n",
      "100 10000 24 tensor(100.)\n",
      "100 10000 25 tensor(100.)\n",
      "100 10000 26 tensor(100.)\n",
      "100 10000 27 tensor(100.)\n",
      "100 10000 28 tensor(100.)\n",
      "100 10000 29 tensor(100.)\n",
      "100 10000 30 tensor(100.)\n",
      "100 10000 31 tensor(100.)\n",
      "100 10000 32 tensor(100.)\n",
      "100 10000 33 tensor(100.)\n",
      "100 10000 34 tensor(100.)\n",
      "100 10000 35 tensor(100.)\n",
      "100 10000 36 tensor(100.)\n",
      "100 10000 37 tensor(100.)\n",
      "100 10000 38 tensor(100.)\n",
      "100 10000 39 tensor(100.)\n",
      "100 10000 40 tensor(100.)\n",
      "100 10000 41 tensor(100.)\n",
      "100 10000 42 tensor(100.)\n",
      "100 10000 43 tensor(100.)\n",
      "100 10000 44 tensor(100.)\n",
      "100 10000 45 tensor(100.)\n",
      "100 10000 46 tensor(100.)\n",
      "100 10000 47 tensor(100.)\n",
      "100 10000 48 tensor(100.)\n",
      "100 10000 49 tensor(100.)\n",
      "100 10000 50 tensor(100.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(5,5))\\n\\nx_data = [i/1000. for i in range(100000, 1, -10)]\\nfor threshold in range(10,0,-1):\\n    label = f\"{threshold*10 : 2d}\"\\n    y_data = [getSoftK(data, i/1000., threshold/10.).item() for i in range(100000, 1, -10)]\\n    plt.plot(x_data, y_data, label=label, linewidth=1.)\\n\\nplt.xscale(\\'log\\')\\nplt.yticks(list(range(11)))\\nplt.xticks([0.01,0.1,1.,10.,100.])\\nplt.xlabel(\"Temperature (SoftMax)\")\\nplt.ylabel(\"Soft indexed items\")\\nplt.grid(color=\\'#CCCCCC\\', linestyle=\\'-\\', linewidth=0.7, zorder=0)\\nplt.legend(fontsize=8, title=\"Threshold\", labelspacing=0.4, title_fontsize=9)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 1286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSoftK(data, temperature, threshold, verbose=False):\n",
    "    S1 = to.softmax(data / temperature, dim=0) # probabilities\n",
    "    if verbose: print (S1)\n",
    "    Sorted, _ = torch.sort(S1, dim=0, descending=True)\n",
    "    if verbose: print (Sorted)\n",
    "    C1 = torch.cumsum(Sorted, dim=0)\n",
    "    if verbose: print (C1)\n",
    "    S2 = torch.sigmoid((C1 - threshold) * 100)\n",
    "    S2 = torch.abs(S2-1)\n",
    "    if verbose: print (S2)\n",
    "    soft_k = S2.sum()\n",
    "    return soft_k + 1\n",
    "\n",
    "K = 5.\n",
    "j = data.size(0)\n",
    "D1 = K - ((j**2) * data_e)\n",
    "D2 = data_e.sum()\n",
    "n = torch.sum(D1/D2)\n",
    "\n",
    "print (n)\n",
    "topMask = getTopKMask(data, a, n)\n",
    "print (topMask.sum())\n",
    "#print (getSoftK(data, temperature, threshold, True))\n",
    "\n",
    "def getTopKMask(data, a, n):\n",
    "    S1 = to.softmax(data, dim=0)\n",
    "    S2 = (S1 * a**2) + n\n",
    "    top_k_mask = torch.sigmoid(S2 * 1.)\n",
    "    return top_k_mask\n",
    "\n",
    "print ('topKMask')\n",
    "for i in range(-int(data.size(0)/2)-1,int(data.size(0)/2)+1):\n",
    "    a = data.size(0) \n",
    "    c = i \n",
    "    topMask = getTopKMask(data, a, c)\n",
    "    #print (topMask)\n",
    "    print (a, b, c, topMask.sum())\n",
    "\n",
    "\n",
    "\n",
    "temperature = 10.\n",
    "threshold = 0.5\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "x_data = [i/1000. for i in range(100000, 1, -10)]\n",
    "for threshold in range(10,0,-1):\n",
    "    label = f\"{threshold*10 : 2d}\"\n",
    "    y_data = [getSoftK(data, i/1000., threshold/10.).item() for i in range(100000, 1, -10)]\n",
    "    plt.plot(x_data, y_data, label=label, linewidth=1.)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yticks(list(range(11)))\n",
    "plt.xticks([0.01,0.1,1.,10.,100.])\n",
    "plt.xlabel(\"Temperature (SoftMax)\")\n",
    "plt.ylabel(\"Soft indexed items\")\n",
    "plt.grid(color='#CCCCCC', linestyle='-', linewidth=0.7, zorder=0)\n",
    "plt.legend(fontsize=8, title=\"Threshold\", labelspacing=0.4, title_fontsize=9)\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "id": "1dbfa621-4f3b-4c7c-bf70-78e6a032f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: tensor([0.1000, 0.6000, 0.2400, 0.1200, 0.4500])\n",
      "Pairwise distances:\n",
      " tensor([[ 0.0000,  0.5000,  0.1400,  0.0200,  0.3500],\n",
      "        [-0.5000,  0.0000, -0.3600, -0.4800, -0.1500],\n",
      "        [-0.1400,  0.3600,  0.0000, -0.1200,  0.2100],\n",
      "        [-0.0200,  0.4800,  0.1200,  0.0000,  0.3300],\n",
      "        [-0.3500,  0.1500, -0.2100, -0.3300,  0.0000]])\n",
      "Soft permutation matrix:\n",
      " tensor([[0.0053, 0.7903, 0.0216, 0.0065, 0.1763],\n",
      "        [0.0053, 0.7903, 0.0216, 0.0065, 0.1763],\n",
      "        [0.0053, 0.7903, 0.0216, 0.0065, 0.1763],\n",
      "        [0.0053, 0.7903, 0.0216, 0.0065, 0.1763],\n",
      "        [0.0053, 0.7903, 0.0216, 0.0065, 0.1763]])\n",
      "Soft sorted data:\n",
      " tensor([0.5600, 0.5600, 0.5600, 0.5600, 0.5600])\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "data = torch.tensor([0.1, 0.6, 0.24, 0.12, 0.45])\n",
    "print(\"Original data:\", data)\n",
    "\n",
    "# Compute pairwise differences\n",
    "distances = data.unsqueeze(0) - data.unsqueeze(1)\n",
    "print(\"Pairwise distances:\\n\", distances)\n",
    "\n",
    "# Compute soft ranking using softmax over rows\n",
    "sm1 = torch.softmax(distances / 0.1, dim=1)  # Negative sign to encourage ranking\n",
    "print(\"Soft permutation matrix:\\n\", sm1)\n",
    "\n",
    "# Compute soft sorted values\n",
    "soft_sorted_data = sm1 @ data\n",
    "print(\"Soft sorted data:\\n\", soft_sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "id": "b1adcc63-1bb2-4a1e-ad67-8d3bf9f4c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3893, 0.6566, 0.8294, 0.3439, 0.2954, 0.6522, 0.5467, 0.2161, 0.9642,\n",
      "        0.2672, 0.3302, 0.4980, 0.4238, 0.1478, 0.5290, 0.8497, 0.6590, 0.7930,\n",
      "        0.1378, 0.8744])\n",
      "\n",
      "sum tensor(10.4037)\n",
      "\n",
      "SoftMax\n",
      "tensor([17.0067, 22.2164, 26.4081, 16.2516, 15.4814, 22.1205, 19.9056, 14.3022,\n",
      "        30.2192, 15.0508, 16.0303, 18.9598, 17.6025, 13.3579, 19.5549, 26.9500,\n",
      "        22.2702, 25.4643, 13.2248, 27.6227]) tensor(400.0000)\n",
      "tensor([-2.4933,  2.7164,  6.9081, -3.2484, -4.0186,  2.6205,  0.4056, -5.1978,\n",
      "        10.7192, -4.4492, -3.4697, -0.5402, -1.8975, -6.1421,  0.0549,  7.4500,\n",
      "         2.7702,  5.9643, -6.2752,  8.1227]) tensor(10.0000)\n",
      "\n",
      "Sigmoid\n",
      "tensor([0.0763, 0.9380, 0.9990, 0.0374, 0.0177, 0.9322, 0.6000, 0.0055, 1.0000,\n",
      "        0.0116, 0.0302, 0.3682, 0.1304, 0.0021, 0.5137, 0.9994, 0.9410, 0.9974,\n",
      "        0.0019, 0.9997]) tensor(9.6017) tensor(28.2801)\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(20)\n",
    "\n",
    "print (data)\n",
    "print ('')\n",
    "print ('sum', data.sum())\n",
    "print ('')\n",
    "print ('SoftMax')\n",
    "\n",
    "sm = torch.softmax(data, dim=-1) * len(data) ** 2\n",
    "print (sm, sm.sum())\n",
    "temp = sm - (len(data) - 0.5)\n",
    "print (temp, temp.sum())\n",
    "\n",
    "print ('')\n",
    "print ('Sigmoid')\n",
    "sig = 1./(1. + (torch.e ** -(temp * 1.)))\n",
    "print (sig, sig.sum(), data.sum() * torch.e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "id": "1cbe2bfb-528f-4451-ba8c-183110d72976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input scores: tensor([[3.0000, 1.0000, 4.0000, 1.5000]])\n",
      "Soft-sorted scores: tensor([[2.5182, 2.5182, 2.5182, 2.5182]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def softsort(scores, tau=1.0):\n",
    "    \"\"\"\n",
    "    Differentiable sorting using a soft permutation matrix.\n",
    "    \n",
    "    Args:\n",
    "        scores (torch.Tensor): Input tensor of shape (batch_size, n), representing scores to be sorted.\n",
    "        tau (float): Temperature parameter controlling the softness of the sorting.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Approximate sorted values with the same shape as scores.\n",
    "    \"\"\"\n",
    "    batch_size, n = scores.shape\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    pairwise_diff = scores.unsqueeze(2) - scores.unsqueeze(1)  # Shape: (batch_size, n, n)\n",
    "    \n",
    "    # Compute soft permutation matrix using softmax\n",
    "    P = F.softmax(-pairwise_diff / tau, dim=-1)  # Shape: (batch_size, n, n)\n",
    "    \n",
    "    # Compute sorted values\n",
    "    sorted_scores = torch.matmul(P, scores.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, n)\n",
    "    \n",
    "    return sorted_scores\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scores = torch.tensor([[3.0, 1.0, 4.0, 1.5]], dtype=torch.float32)  # Example input\n",
    "    tau = 10.  # Lower tau makes it closer to hard sorting\n",
    "    sorted_scores = softsort(scores, tau=tau)\n",
    "    print(\"Input scores:\", scores)\n",
    "    print(\"Soft-sorted scores:\", sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "id": "c54e6067-617f-4c38-85b5-8b7f600eb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.pytorch_ops import soft_rank, soft_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "id": "5202937c-4658-4dae-a6c2-50145c2ef661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8, 4, 6, 0, 7, 1, 9, 3]\n",
      "tensor([[4.4100, 4.4300, 4.4500, 4.4700, 4.4900, 4.5100, 4.5300, 4.5500, 4.5700,\n",
      "         4.5900]], dtype=torch.float64, grad_fn=<StackBackward0>)\n",
      "tensor([[3.6000, 3.8000, 4.0000, 4.2000, 4.4000, 4.6000, 4.8000, 5.0000, 5.2000,\n",
      "         5.4000]], dtype=torch.float64, grad_fn=<StackBackward0>)\n",
      "tensor([[-0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=torch.float64,\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "raw = list(range(10))\n",
    "random.shuffle(raw)\n",
    "data = torch.tensor(raw, dtype=torch.float32, requires_grad=True)\n",
    "print (raw)\n",
    "\n",
    "soft_sorted_data = soft_sort(data.unsqueeze(0), regularization_strength=50.0)\n",
    "print (soft_sorted_data)\n",
    "soft_sorted_data = soft_sort(data.unsqueeze(0), regularization_strength=5.0)\n",
    "print (soft_sorted_data)\n",
    "soft_sorted_data_2 = soft_sort(data.unsqueeze(0), regularization_strength=0.01)\n",
    "print (soft_sorted_data_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798f0f8-3dca-4128-9afa-4eb47fdbdfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
